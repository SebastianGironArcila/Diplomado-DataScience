{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoyl4nLf9Pj3"
   },
   "source": [
    "# Árboles de decisión: Churn compañía de servicios de telefonía móvil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeMBQ-OA9Pj9"
   },
   "source": [
    "Vamos a aplicar un modelo de clasificación de árboles de decisión a un dataset que describe los clientes de una compañía que presta servicios de telefonía móvil que se han abandonado o no la compañía para irse a la competencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHvQ0tQn9Pj9"
   },
   "source": [
    "Importamos las librerías que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3r1yLE19Pj9"
   },
   "outputs": [],
   "source": [
    "import numpy as np #operaciones matriciales y con vectores\n",
    "import pandas as pd #tratamiento de datos\n",
    "import matplotlib.pyplot as plt #gráficos\n",
    "from sklearn import tree, datasets, metrics\n",
    "#from sklearn import neighbors, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split #metodo de particionamiento de datasets para evaluación\n",
    "from sklearn.model_selection import cross_val_score, cross_validate #método para evaluar varios particionamientos de C-V\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, LeaveOneOut #Iteradores de C-V\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--7aRt3b9Pj-"
   },
   "source": [
    "## Entendimiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvbgrbQ89Pj-"
   },
   "source": [
    "Cargamos los datos para entenderlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "zof35mEu9Pj-",
    "outputId": "6012c5d8-daf2-4eaf-ab08-21668fbc7b4a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('01_churn.csv', sep=';', na_values=\".\")\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uu_ym1kr9Pj_"
   },
   "source": [
    "**Parte 1**:\n",
    "* Determine el número de registros, de variables, sus tipos ideales/reales, y sus rangos\n",
    "* Determine el baseline (porcentaje clase mayoritaria)y su accuracy. ¿Están balanceados los datos (use diagrama de barras)?\n",
    "* ¿Encuentran algún problema con los datos (missing values, datos inválidos, etc.)?\n",
    "* Si fuésemos a utilizar K-NN, ¿debería hacerse algún pretratamiento de los datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9jQ4OER9Pj_"
   },
   "source": [
    "El método **describe** permite obtener un dataframe con una descripción de las variables de un dataframe analizado. Para cada variable encontramos el número de registros validos (*count*).\n",
    "\n",
    "Además, si se trata de una variable categórica, se puede obtener el número de clases posibles (*unique*), la clase mayoritaria (*top*) y la frecuencia de la clase mayoritaria (*freq*).\n",
    "\n",
    "Y, si se trata de una variable numérica, se puede obtener el promedio (*mean*), desviación estándar (*std*), los valores mínimos (*min*) y máximos (*max*) y los cuartiles (*25%*, *50%* y *75%*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCiLGqESAvcS"
   },
   "outputs": [],
   "source": [
    "print(data.describe(include='all'))\n",
    "print(data.dtypes)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=data[\"LEAVE\"], columns=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline sobre la variable objetivo\n",
    "data['LEAVE'][data['LEAVE'] == 'STAY'].count()/data.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(x='LEAVE',data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R//  Si fueramos a utiliar K-NN antes de entrenar el modelo, deberiamos escalar los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIjRU3A69PkB"
   },
   "source": [
    "**Parte 2**:\n",
    "\n",
    "Utilizando pandas y matplotlib, analice la distribución de las variables independientes con respecto a los valores de la variable objetivo LEAVE y STAY. Trate de encontrar patrones en plots univariados (densidad) y Bivariados (scatterplots) para las variables numéricas, y gráficos de barras de conteo para las categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4FfVcJZ9PkB"
   },
   "outputs": [],
   "source": [
    "var_indep_cat = ['COLLEGE', 'REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL',\n",
    "       'CONSIDERING_CHANGE_OF_PLAN', 'LEAVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TggG4IPN9PkB"
   },
   "outputs": [],
   "source": [
    "var_indep_num = ['INCOME', 'OVERAGE', 'LEFTOVER', 'HOUSE', 'HANDSET_PRICE',\n",
    "                 'OVER_15MINS_CALLS_PER_MONTH', 'AVERAGE_CALL_DURATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBrbER2C9PkB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2, figsize=(16,15))\n",
    "sns.kdeplot(data = data, x = 'INCOME', hue='LEAVE', ax=axes[0,0], fill = True)\n",
    "sns.kdeplot(data = data, x = 'OVERAGE', hue='LEAVE',ax=axes[0,1], fill = True)\n",
    "sns.kdeplot(data = data, x = 'LEFTOVER',  hue='LEAVE', ax=axes[1,0], fill = True)\n",
    "sns.kdeplot(data = data, x = 'HOUSE',  hue='LEAVE', ax=axes[1,1], fill = True)\n",
    "sns.kdeplot(data = data, x = 'HANDSET_PRICE', hue='LEAVE', ax=axes[2,0], fill = True)\n",
    "sns.kdeplot(data = data, x = 'OVER_15MINS_CALLS_PER_MONTH', hue='LEAVE', ax=axes[2,1], fill = True)\n",
    "sns.kdeplot(data = data, x = 'AVERAGE_CALL_DURATION',  hue='LEAVE', ax=axes[3,0], fill = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_indep_num = ['INCOME', 'OVERAGE', 'LEFTOVER', 'HOUSE', 'HANDSET_PRICE',\n",
    "                 'OVER_15MINS_CALLS_PER_MONTH', 'AVERAGE_CALL_DURATION', 'LEAVE']\n",
    "g = sns.PairGrid(data[var_indep_num], hue=\"LEAVE\")\n",
    "g.map_offdiag(sns.scatterplot)\n",
    "g.map_diag(sns.kdeplot, lw=3)\n",
    "g.add_legend()\n",
    "\n",
    "#jitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d2 = data[[\"OVER_15MINS_CALLS_PER_MONTH\", \"AVERAGE_CALL_DURATION\", \"LEAVE\"]]\n",
    "jitter = 0.3\n",
    "d2.OVER_15MINS_CALLS_PER_MONTH = data.OVER_15MINS_CALLS_PER_MONTH + np.random.normal(scale=jitter, size=20000)\n",
    "d2.AVERAGE_CALL_DURATION = data.AVERAGE_CALL_DURATION + np.random.normal(scale=jitter, size=20000)\n",
    "\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"OVER_15MINS_CALLS_PER_MONTH\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=d2, ax=ax, size=1)\n",
    "plt.title(\"OVER_15MINS_CALLS_PER_MONTH vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaBfOy3l9PkD"
   },
   "source": [
    "# Clasificación a partir un árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaCZMME9PkD"
   },
   "source": [
    "Los árboles de decisión permiten utilizar tanto las variables predictivas categóricas como las numéricas.\n",
    "No todas las variables van a ser útiles. El árbol se va a encargar de encontrar la mejor variable a utilizar en el contexto del subconjunto de datos de cada rama.\n",
    "\n",
    "Veamos cómo se crea un árbol de decisión en scikit-learn, y algunos de los parámetros más importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNSDbb2k9PkE"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos (gini, entropy)\n",
    "    max_depth=None,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "    min_samples_split=2, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "    min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "    max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight=None    # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNSql9El9PkE"
   },
   "source": [
    "**Nota editorial**: scikit-learn soporta en sus árboles de decisión solamente variables independientes numéricas!!!!! Toca entonces utilizar un encoding (dummies). Realicelo para proceder a entrenar..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAEQmnKjCf6B"
   },
   "outputs": [],
   "source": [
    "# no debe aparecer la variable objetivo\n",
    "dataDummies = data.drop('LEAVE',axis=1)\n",
    "dataDummies = pd.get_dummies(dataDummies)\n",
    "dataDummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoaOb0929PkE"
   },
   "source": [
    "Quedarían 24 variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kU4a417x9PkE"
   },
   "outputs": [],
   "source": [
    "#utilice el metodo train_test_split\n",
    "from sklearn.model_selection import train_test_split #metodo de particionamiento de datasets para evaluación\n",
    "X = dataDummies\n",
    "y = data['LEAVE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et5Agc-59PkE",
    "outputId": "d0f7f3f3-c37a-4652-93ea-120e875e743d"
   },
   "outputs": [],
   "source": [
    "ctree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKoAqlTb9PkE"
   },
   "source": [
    "Una vez el árbol es aprendido, se puede consultar diferentes atributos.\n",
    "El mas interesante, aparte del árbol en sí, es el que asocia un índice de importancia a los atributos independientes en la clasificación (Traten de graficar un diagrama de barras con los pesos y las variables de mayor a menor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4j5BPQ59PkE"
   },
   "outputs": [],
   "source": [
    "ctree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khvyjSMp9PkE"
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awExn9ux9PkE"
   },
   "source": [
    "Encontramos entonces que las variables en orden de importancia son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv8KK0jT9PkE",
    "outputId": "51666dd9-22e5-456b-d536-c92f450961e6"
   },
   "outputs": [],
   "source": [
    "X.columns[np.argsort(-ctree.feature_importances_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(X.columns, ctree.feature_importances_)))\n",
    "df.sort_values(by=1,ascending = False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x=0, y=1, data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiJuqOJJ9PkE"
   },
   "source": [
    "Vamos ahora a visualizar el árbol aprendido (para hacerlo, se necesita preinstalar la aplicación graphviz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC7ytDd39PkE"
   },
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from IPython.display import Image  \n",
    "#from sklearn.externals.six import StringIO  \n",
    "from six import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UR3IRNMq9PkE"
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(ctree, \n",
    "                     filled=True, rounded=True,  #nodos redondeados y coloreados\n",
    "                     class_names=ctree.classes_,\n",
    "                     feature_names=X_train.columns,  \n",
    "                     out_file=dot_data,\n",
    "                     special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McrQgfyL9PkE"
   },
   "source": [
    "La imágen es demasiado grande para poder visualizarla aquí, vamos a guardarla un archivo y abrirla en un visor externo que permita hacer zoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOmTMr4Z9PkE"
   },
   "outputs": [],
   "source": [
    "# Create PNG\n",
    "graph.write_png(\"arbol.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfaeDUhL9PkF"
   },
   "source": [
    "Como se puede ver el árbol es increiblemente grande y complejo, pues no se especificó ninguna manera de limitar su crecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz9FE4nq9PkF"
   },
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaY4ii059PkF"
   },
   "source": [
    "Ya tenemos el modelo \"aprendido\" con el dataset de 20000 instancias.\n",
    "Vamos ahora a evaluarlo sobre ese mismo dataset para poder ver los éxitos y errores de la predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U1C8xzN9PkF"
   },
   "outputs": [],
   "source": [
    "y_pred = ctree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8EOXvLq9PkF"
   },
   "outputs": [],
   "source": [
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de confusión para K=5\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, y_test.unique())\n",
    "plt.yticks(tick_marks, y_test.unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32W-l2309PkF"
   },
   "outputs": [],
   "source": [
    "print(cm)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Precisión     : \", metrics.precision_score(y_test, y_pred, \"LEAVE\", average='macro'))\n",
    "print(\"Recall        : \", metrics.recall_score(y_test, y_pred, \"LEAVE\", average='macro'))\n",
    "VN = np.sum(cm[1:3,1:3])\n",
    "FP = np.sum(cm[0,1:3])\n",
    "specificity = VN/(VN+FP)\n",
    "print(\"Especificidad : \", specificity)\n",
    "print(\"F1-score      : \", metrics.f1_score(y_test, y_pred, \"LEAVE\", average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Clcuud9PkF"
   },
   "source": [
    "Con el árbol completo, obtuvimos un nivel de accuracy del 61.68%, cuando el baseline era de 50.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1c0HulB9PkF"
   },
   "source": [
    "## Overfitting: poda del árbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAEln0ag9PkF"
   },
   "source": [
    "Vamos a ver que hubiera pasado si limitamos el crecimiento del árbol (prepoda), controlando la profundidad del árbol y el mínimo número de registros de un nodo para permitir el particionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb9ks28t9PkF"
   },
   "source": [
    "**Parte 3**:\n",
    "\n",
    "Modifiquen el parámetro **criterion** y los parámetros de pre-poda buscando una mejor exactitud del modelo:\n",
    "- **max_depth**: entre mas grande el valor, el árbol será más complejo (más número de niveles de profundidad)\n",
    "- **min_samples_split**: entre mas grande el valor, el árbol será más sencillo (se necesita tener más registros en un nodo para poder particionarlo)\n",
    "- **min_samples_leaf**: entre mas grande el valor, el árbol será más sencillo (se necesita tener más registros en una hoja para poder aceptarla, si no se llega a esa cardinalidad, no se permite el partionamiento de su nodo padre) \n",
    "- **min_impurity_decrease**: entre mas grande el valor, el árbol será más sencillo (un nivel de impureza bajo inferior a este umbral no desatará un particionamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF9rjIF69PkF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "    max_depth=30,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "    min_samples_split=20, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "    min_samples_leaf=20,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "    max_leaf_nodes=20, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight='balanced'  # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")\n",
    "ctree.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3znfNzR9PkF"
   },
   "outputs": [],
   "source": [
    "y_pred = ctree.predict(X_test)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAom_y0g9PkF"
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(ctree, \n",
    "                     filled=True, rounded=True,  #nodos redondeados y coloreados\n",
    "                     class_names=ctree.classes_,\n",
    "                     feature_names=X_train.columns,  \n",
    "                     out_file=dot_data,\n",
    "                     special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84KN2S9M9PkF"
   },
   "source": [
    "Podríamos seguir mejorando el árbol buscando un tuning de la prepoda mas complejo con los demás parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWo1bZc29PkF"
   },
   "source": [
    "Podemos intentar con otro tipo de modelos, e.g KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTkrkco89PkF"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(1234)\n",
    "knn = KNeighborsClassifier(n_neighbors=300)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV1dG6Vl9PkF"
   },
   "source": [
    "También con un modelo Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dquF2gfJ9PkF"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "np.random.seed(1234)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9YHeA139PkG"
   },
   "source": [
    "## Modelos de ensamble: bagging, random forest, boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DF5t1Qy9PkG"
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0ohqADx9PkG"
   },
   "source": [
    "Vamos ahora a crear un modelo de ensamble que utiliza muchos modelos de árboles sencillos que pone a votar para encontrar una decisión consensuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN8o2V0t9PkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "    max_depth=7,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "    min_samples_split=1000, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "    min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "    max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight=None    # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")\n",
    "bagging = BaggingClassifier(base_estimator = ctree,   # Por defecto un decision tree \n",
    "                            n_estimators=200,          # Número de modelos a crear\n",
    "                            max_samples=0.7,          # Número o % de registros de la muestra de aprendizaje\n",
    "                            max_features=0.7,         # Número o % de atributos de la muestra de aprendizaje\n",
    "                            bootstrap=True,           # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                            bootstrap_features=False, # Utilizar reemplazo en el muestreo de los atributos de aprendizaje\n",
    "                            oob_score=False,          # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                            n_jobs=2,                 # Número de cores a utilizar\n",
    "                            random_state=None,        # random seed para el generador aleatorio\n",
    "                            verbose=0)                # controla la cantidad de información a reportar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xmnoUQj9PkG"
   },
   "outputs": [],
   "source": [
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08v_pNoL9PkG"
   },
   "source": [
    "Podemos hacer bagging de otro tipo de estimadores de base, por ejemplo KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca3kUd0X9PkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(1234)\n",
    "bagging = BaggingClassifier(base_estimator = KNeighborsClassifier(),   # Por defecto un decision tree \n",
    "                            n_estimators=50,          # Número de modelos a crear\n",
    "                            max_samples=0.5,          # Número o % de registros de la muestra de aprendizaje\n",
    "                            max_features=0.5,         # Número o % de atributos de la muestra de aprendizaje\n",
    "                            bootstrap=True,           # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                            bootstrap_features=False, # Utilizar reemplazo en el muestreo de los atributos de aprendizaje\n",
    "                            oob_score=False,          # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                            n_jobs=2,                 # Número de cores a utilizar\n",
    "                            random_state=None,        # random seed para el generador aleatorio\n",
    "                            verbose=0)                # controla la cantidad de información a reportar\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmRbmJ_x9PkG"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yyx-nv4-9PkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(1234)\n",
    "rforest = RandomForestClassifier(n_estimators=100,     #Número de modelos a crear \n",
    "                                criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "                                max_depth=6,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "                                min_samples_split=1000, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "                                min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "                                max_features=('auto'), # Número o % de atributos de la muestra de aprendizaje\n",
    "                                max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "                                min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "                                bootstrap=True,      # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                                oob_score=True,     # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                                n_jobs=2,            # Número de cores a utilizar\n",
    "                                random_state=None,   # random seed para el generador aleatorio\n",
    "                                verbose=0)           # controla la cantidad de información a reportar\n",
    "rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMEUZo1s9PkG"
   },
   "outputs": [],
   "source": [
    "rforest.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVfozCpn9PkG"
   },
   "source": [
    "Encontramos que las variables en orden de importancia son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JCt_bxC9PkG"
   },
   "outputs": [],
   "source": [
    "X.columns[np.argsort(-rforest.feature_importances_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD7bpYDc9PkG"
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RjvOiyc9PkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "np.random.seed(1234)\n",
    "\n",
    "numModelos = 400\n",
    "ada_10 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                         algorithm='SAMME',\n",
    "                         learning_rate=1.0)    #Reduce la importancia de los modelos mas recientes\n",
    "ada_10.fit(X_train, y_train)\n",
    "ada_02 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                        algorithm='SAMME',\n",
    "                         learning_rate=0.2)    #Reduce la importancia de los modelos mas recientes\n",
    "\n",
    "ada_02.fit(X_train, y_train)\n",
    "ada_005 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                         algorithm='SAMME',\n",
    "                         learning_rate=0.05)    #Reduce la importancia de los modelos mas recientes\n",
    "ada_005.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eT20oROz9PkG"
   },
   "outputs": [],
   "source": [
    "ada_errores_test_10 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_10.staged_predict(X_test)):\n",
    "    ada_errores_test_10[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "ada_errores_train_10 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_10.staged_predict(X_train)):\n",
    "    ada_errores_train_10[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC5qER8m9PkG"
   },
   "outputs": [],
   "source": [
    "ada_errores_test_02 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_02.staged_predict(X_test)):\n",
    "    ada_errores_test_02[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "ada_errores_train_02 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_02.staged_predict(X_train)):\n",
    "    ada_errores_train_02[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk3WcRrE9PkG"
   },
   "outputs": [],
   "source": [
    "ada_errores_test_005 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_005.staged_predict(X_test)):\n",
    "    ada_errores_test_005[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "\n",
    "ada_errores_train_005 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_005.staged_predict(X_train)):\n",
    "    ada_errores_train_005[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NxQZ9iB9PkG"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_10,\n",
    "        label='AdaBoost Test Acc lr 1.0',\n",
    "        color='red')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_10,\n",
    "        label=' AdaBoost Train Acc lr 1.0',\n",
    "        color='blue')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_02,\n",
    "        label='AdaBoost Test Acc lr 0.2',\n",
    "        color='gray')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_02,\n",
    "        label=' AdaBoost Train Acc lr 0.2',\n",
    "        color='cyan')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_005,\n",
    "        label='AdaBoost Test Acc lr 0.05',\n",
    "        color='green')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_005,\n",
    "        label=' AdaBoost Train Acc lr 0.05',\n",
    "        color='yellow')\n",
    "\n",
    "ax.set_ylim((0.6, 0.7))\n",
    "ax.set_xlabel('# de modelos')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "leg = ax.legend(loc='lower right', fancybox=True)\n",
    "leg.get_frame().set_alpha(0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNClQBHO9PkH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "m9YHeA139PkG"
   ],
   "name": "01_Arboles_Churn-STUD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
